<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    <meta name="author" content="Andi Gu" />
      <meta name="dcterms.date" content="2022-06-28" />
      <meta name="keywords" content="geometric algebra, notes,
physics" />
      <meta name="description" content="Geometric algebra notes from the Geometric Algebra for Physicists textbook." />
    <meta name="google-site-verification" content="UXM-onlEexVMyioOC6QQ4F9dBlEU_IAxuc-7gWPiwYs" />

  <link rel="author" href="https://scholar.google.com/citations?user=XGTtMs8AAAAJ" />

  <title>Geometric Algebra</title>
  <!-- <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: blue!50!black;
    }
    a:visited {
      color: blue!50!black;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style> -->
    <link rel="stylesheet" href="/static/css/index.css" />
      
    
    
    
    
    
    <style>
    .statement.plain {
    	font-style: italic;
    }
    .statement.plain .statement-label {
    	font-style: normal; font-weight: normal; font-variant: normal; font-weight: bold;
    }
    .statement.plain .statement-spah {
    	word-spacing: 1em;
    }
    .statement.plain .statement-info {
    	font-style: normal; font-weight: normal; font-variant: normal;
    }
    </style>
    <style>
    .statement.proof {
    	font-style: normal; font-weight: normal; font-variant:normal;
    }
    .statement.proof .statement-label {
    	font-style: normal; font-weight: normal; font-variant: normal; font-style: italic;
    }
    .statement.proof .statement-spah {
    	word-spacing: 1em;
    }
    .statement.proof .statement-info {
    	font-style: normal; font-weight: normal; font-variant: normal;
    }
    </style>
  
  <script>
    window.MathJax = {
      loader: { load: ['[tex]/physics', '[tex]/mathtools'] },
      tex: { packages: { '[+]': ['physics', 'mathtools'] } }
    };

  </script>

    <script
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
    type="text/javascript"></script>
    <script type="module" src="/static/js/js.cookie.min.js"></script>
  <script src="https://kit.fontawesome.com/de9cc10f80.js" crossorigin="anonymous"></script>

  <script src="/static/js/script.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-176088249-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-176088249-1');
  </script>


</head>

<body>
  
  <div class="container">

    <div class="nav-container">
      <nav class="navbar">
        <a href="/" class="nav-link" id="nav-home">Home</a>
        <a href="/notes" class="nav-link" id="nav-notes">Notes</a>
      </nav>
    </div>

    <div class="content-container">
            <header id="title-block-header">
        <h1 class="title">Geometric Algebra</h1>
                        <p class="author">Andi Gu</p>
        
              </header>
                  <nav id="TOC" role="doc-toc">
                <h2 id="toc-title">Table of Contents</h2>
                <ul>
                <li><a href="#foundations" id="toc-foundations"><span
                class="toc-section-number">1</span> Foundations</a>
                <ul>
                <li><a href="#the-dot-and-wedge-product"
                id="toc-the-dot-and-wedge-product"><span
                class="toc-section-number">1.1</span> The Dot and Wedge
                Product</a></li>
                <li><a href="#a-few-additional-operations"
                id="toc-a-few-additional-operations"><span
                class="toc-section-number">1.2</span> A Few Additional
                Operations</a></li>
                <li><a href="#rotations-and-reflections"
                id="toc-rotations-and-reflections"><span
                class="toc-section-number">1.3</span> Rotations and
                Reflections</a></li>
                <li><a href="#frames" id="toc-frames"><span
                class="toc-section-number">1.4</span> Frames</a></li>
                </ul></li>
                <li><a href="#bibliography"
                id="toc-bibliography">References</a></li>
                </ul>
      </nav>
            <p>This is a collection of notes from my study of the
            <em>Geometric Algebra for Physicists</em> textbook <span
            class="citation" data-cites="doran2007">(<a
            href="#ref-doran2007" role="doc-biblioref">Doran and Lasenby
            2007</a>)</span>.</p>
            <h1 data-number="1" id="foundations"><span
            class="header-section-number">1</span> Foundations</h1>
            <p>The entire structure of geometric algebra can be boiled
            down to the geometric product, which is a binary operation
            between two vectors <span class="math inline">\(a\)</span>
            and <span class="math inline">\(b\)</span>. The usual rules
            of a vector space apply to these vectors, and the usefulness
            of geometric algebra is in the structure revealed by the
            product <span class="math inline">\(ab\)</span>. This
            product satisfies three fairly natural axioms.</p>
            <ol type="1">
            <li>It is associative: <span
            class="math inline">\((ab)c=a(bc)\)</span></li>
            <li>It distributes over addition: <span
            class="math inline">\(a(b+c)=ab+ac\)</span></li>
            <li>The square of any vector is real: <span
            class="math inline">\(a^2 \in \mathbb{R}\)</span>. We do not
            make the assumption that <span class="math inline">\(a^2
            \geq 0\)</span> to allow for the possibility of spaces with
            mixed signature (e.g., Minkowski space).</li>
            </ol>
            <p>It is important not to interpret this product <span
            class="math inline">\(uv\)</span> as another vector. In
            fact, <span class="math inline">\(uv\)</span> is a sum of a
            scalar element <span class="math inline">\(u \in
            \mathbb{R}\)</span> and a ‘bivector’ <span
            class="math inline">\(V\)</span>: <span
            class="math inline">\(ab=u + V\)</span> – this should be
            understood as something like a complex number <span
            class="math inline">\(u+iv\)</span>. In fact, for a vector
            space of dimension 2, this analogy is exact. In general, the
            bivector should be pictured as a directed area, or an object
            like the angular momentum in classical mechanics. This
            analogy extends further: we can have a trivector, which is
            an oriented volume, and so on and so forth.</p>
            <p>However, this geometric product is not so exotic. We can
            still understand it in terms of some familiar concepts. For
            one, we observe that <span
            class="math inline">\((a+b)^2\)</span> is always real, so we
            must have <span class="math inline">\(a^2+ab+ba+b^2 \in
            \mathbb{R} \implies ab + ba \in \mathbb{R}\)</span>. We have
            found that the symmeterized geometric product is a binary
            relation between vectors that outputs a real number – that
            is, a dot product! So we define <span
            class="math display">\[a \cdot b =
            \frac{(ab+ba)}{2}.\]</span> We define the remaining part of
            the product as the exterior product (which might be thought
            of as replacing the role of the cross product): <span
            class="math display">\[a \wedge b =
            \frac{ab-ba}{2}.\]</span> This then gives <span
            class="math inline">\(ab=a \cdot b + a \wedge b\)</span>: a
            sum of a scalar and a bivector, as promised.</p>
            <p>Well, technically, not yet: we do not have a good reason
            to believe we should understand <span
            class="math inline">\(a \wedge b\)</span> as a bivector (or
            a directed area) yet. So we define it as such, and show this
            definition conforms to our intuition later. More precisely,
            we define exterior product of <span
            class="math inline">\(r\)</span> vectors as the full
            anti-symmeterized product over all of them: <span
            class="math display">\[a_1 \wedge a_2 \wedge \ldots \wedge
            a_r = \frac{1}{r!}\sum_{\sigma \in \mathbb{S}_r}
            (-1)^{\sigma} a_{\sigma(1)} a_{\sigma(2)} \ldots
            a_{\sigma(r)},\]</span> where <span
            class="math inline">\(\mathbb{S}_r\)</span> is the set of
            all permutations on <span class="math inline">\(r\)</span>
            elements and <span
            class="math inline">\((-1)^\sigma\)</span> is the sign of a
            permutation. A consequence of this definition is that the
            product reverses sign under exchange of any two vectors.
            Then, we see that if any vector is repeated, the exterior
            product must be zero, which then implies if the vectors are
            linearly dependent, the exterior product is also zero (this
            follows simply by distributivity of the geometric product
            under addition). Therefore, the outer product can be
            understood to measure the dimensionality of a set of
            vectors. We say that the outer product of <span
            class="math inline">\(r\)</span> vectors has grade <span
            class="math inline">\(r\)</span> (if it does not vanish). A
            multivector that can be written purely as an outer product
            is called a blade.</p>
            <div id="blade-product" class="statement lemma plain">
            <p><span class="statement-heading"><span
            class="statement-label">Lemma 1</span>.</span><span
            class="statement-spah"> </span>Any blade <span
            class="math inline">\(a_1 \wedge \ldots \wedge a_r\)</span>
            can be written simply as a product of orthogonal vectors
            <span class="math inline">\(e_1 \ldots e_r\)</span>. This
            justifies our idea that a blade with <span
            class="math inline">\(r\)</span> vectors can be interpreted
            as a directed area for <span
            class="math inline">\(r=2\)</span>, volume for <span
            class="math inline">\(r=3\)</span>, and so on.</p>
            </div>
            <div class="statement proof proof unnumbered">
            <p><span class="statement-heading"><span
            class="statement-label">Proof</span>.</span><span
            class="statement-spah"> </span>Let <span
            class="math inline">\(A_{i,j}=a_i \cdot a_j\)</span>. Since
            <span class="math inline">\(A_{i,j}\)</span> is a symmetric
            matrix, it can be diagonalized with <span
            class="math inline">\(P^T D P\)</span>, where <span
            class="math inline">\(P\)</span> is orthogonal. Letting
            <span class="math inline">\(e_i= P_{i,k} a_k\)</span>, we
            see that <span class="math inline">\(e_i \cdot e_j = P_{i,k}
            P_{j,\ell} a_k \cdot a_\ell = P_{i,k} A_{k,\ell}
            (P^T)_{\ell,j} = D_{i,j}\)</span>. Therefore, these vectors
            obey <span class="math inline">\(e_i e_j = -e_j e_i\)</span>
            for <span class="math inline">\(i \neq j\)</span>. Finally,
            since <span class="math inline">\(e_i = P_{i,k} a_k \implies
            a_k = P_{i,k} e_i\)</span>: <span class="math display">\[
            a_1 \wedge \ldots \wedge a_r = \frac{1}{r!}\sum_{\sigma \in
            \mathbb{S}_r} (-1)^\sigma P_{i_1,\sigma(1)}
            P_{i_2,\sigma(2)} \ldots P_{i_r,\sigma(r)} e_{i_1} e_{i_2}
            \ldots e_{i_r}
            \]</span> Note that we have the restriction <span
            class="math inline">\(i_1 \neq i_2 \neq \ldots \neq
            i_r\)</span> due to the antisymmeterization of the sum.
            Therefore, we can rewrite as: <span
            class="math display">\[\begin{align*}
            a_1 \wedge \ldots \wedge a_r &amp;=
            \frac{1}{r!}\sum_{\rho,\sigma \in \mathbb{S}_r} (-1)^\sigma
            P_{\rho(1),\sigma(1)} P_{\rho(2),\sigma(2)} \ldots
            P_{\rho(r),\sigma(r)} e_{\rho(1)} e_{\rho(2)} \ldots
            e_{\rho(r)} \\
            &amp;= \frac{1}{r!}\sum_{\rho,\sigma \in \mathbb{S}_r}
            (-1)^{\rho^{-1}}(-1)^\sigma P_{1,\sigma(\rho^{-1}(1))}
            P_{2,\sigma(\rho^{-1}(2))} \ldots P_{r,\sigma(\rho^{-1}(r))}
            e_{1} e_{2} \ldots e_{r} \\
            &amp;= \sum_{\alpha \in \mathbb{S}_r} (-1)^{\alpha}
            P_{1,\alpha(1)} P_{2,\alpha(2)} \ldots P_{r,\alpha(r)} e_{1}
            e_{2} \ldots e_{r} \\
            &amp;= \det(P) e_1 \ldots e_r
            \end{align*}\]</span> In the second line, the factor <span
            class="math inline">\((-1)^{\rho^{-1}}\)</span> comes from
            unshuffling <span class="math inline">\(e_{\rho(1)} \ldots
            e_{\rho(r)} \rightarrow e_1 \ldots e_r\)</span>, and in the
            third line, we use the fact that for any function <span
            class="math inline">\(f\)</span> that depends only on <span
            class="math inline">\(\sigma \circ \rho^{-1}\)</span>: <span
            class="math inline">\(\sum_{\rho, \sigma \in \mathbb{S}_r}
            f(\sigma \circ \rho^{-1}) = r! \sum_{\alpha \in
            \mathbb{S}_r} f(\alpha)\)</span>, where <span
            class="math inline">\(\alpha\)</span> plays the role of
            <span class="math inline">\(\sigma \circ \rho^{-1}\)</span>.
            Now, observe that <span class="math inline">\(\det(P)= \pm
            1\)</span>, and in the case where <span
            class="math inline">\(\det(P)=-1\)</span>, we can simply
            reorder <span class="math inline">\(e_1, \ldots,e_r\)</span>
            to get rid of the negative sign.</p>
            </div>
            <p>In general, multivectors can be comprised of elements
            with different grades. For a set of orthogonal vectors, we
            say that <span class="math inline">\(e_i\)</span> has grade
            1, <span class="math inline">\(e_i e_j\)</span> (for <span
            class="math inline">\(i \neq j\)</span>) has grade 2, and so
            on. We define <span
            class="math inline">\(\expval{\cdot}_r\)</span> to be the
            component of a multivector with grade <span
            class="math inline">\(r\)</span>, so that in general a
            multivector <span class="math inline">\(A\)</span> in a
            geometric algebra <span
            class="math inline">\(\mathcal{G}_n\)</span> (whose
            underlying vector space has dimension <span
            class="math inline">\(n\)</span>) can be written <span
            class="math inline">\(A=\sum_{r=0}^n \expval{A}_r\)</span>.
            A multivector that satisfies <span
            class="math inline">\(\expval{A}_r=A\)</span> (for some
            <span class="math inline">\(r\)</span>) is called
            homogenous.</p>
            <p>We denote the subspace of <span
            class="math inline">\(\mathcal{G}_n\)</span> of grade <span
            class="math inline">\(r\)</span> as <span
            class="math inline">\(\mathcal{G}_n^r\)</span>. The
            dimensionality of <span
            class="math inline">\(\mathcal{G}_n^r\)</span> is <span
            class="math inline">\(\binom{n}{r}\)</span>, because the
            basis of <span
            class="math inline">\(\mathcal{G}_n^r\)</span> can be formed
            by choosing <span class="math inline">\(r\)</span> items
            from the <span class="math inline">\(n\)</span> basis
            vectors.<span
            class="sidenote-wrapper"><label for="sn-0" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-0" class="margin-toggle"/><span
            class="sidenote">It is important to note that not every
            multivector in <span
            class="math inline">\(\mathcal{G}_n^r\)</span> is a blade.
            The simplest nontrivial example is <span
            class="math inline">\(e_1 e_2 + e_3 e_4\)</span> in <span
            class="math inline">\(\mathcal{G}_n^4\)</span> – there is
            simply no way to write this as a blade because <span
            class="math inline">\({e_1,e_2}\)</span> and <span
            class="math inline">\({e_3,e_4}\)</span> are orthogonal to
            each other.<br />
            <br />
            </span></span> The overall dimensionality is therefore <span
            class="math inline">\(\sum_{r=0}^n
            \binom{n}{r}=2^n\)</span>.</p>
            <h2 data-number="1.1" id="the-dot-and-wedge-product"><span
            class="header-section-number">1.1</span> The Dot and Wedge
            Product</h2>
            <p>We now study the behavior of <span
            class="math inline">\(a B_r\)</span>, where <span
            class="math inline">\(a\)</span> is some vector and <span
            class="math inline">\(B_r\)</span> is a homogenous
            multivector of grade <span class="math inline">\(r\)</span>.
            More specifically, we will show that we can define <span
            class="math inline">\(a \cdot B_r\)</span> and <span
            class="math inline">\(a \wedge B_r\)</span> in terms of
            <span class="math inline">\(aB_r\)</span> and <span
            class="math inline">\(B_r a\)</span> in a way that is
            similar to the original definition for <span
            class="math inline">\(\cdot\)</span> and <span
            class="math inline">\(\wedge\)</span> between two ordinary
            vectors.</p>
            <div id="dot" class="statement theorem plain">
            <p><span class="statement-heading"><span
            class="statement-label">Theorem 2</span>.</span><span
            class="statement-spah"> </span>For any <span
            class="math inline">\(a \in \mathcal{G}_n^1\)</span> and
            <span class="math inline">\(B_r \in
            \mathcal{G}_n^r\)</span>, <span class="math inline">\(\qty(a
            B_r - (-1)^r B_r a)\)</span> is a homogenous multivector
            with grade <span class="math inline">\(r-1\)</span>. This
            motivates the definition <span id="eq:dot"><span
            class="math display">\[
            a \cdot B_r \coloneqq \frac{1}{2} (aB_r - (-1)^r B_r a),
            \tag{1}\]</span></span> so that <span
            class="math inline">\(\cdot\)</span> is a grade-lowering
            operation.</p>
            </div>
            <div class="statement proof proof unnumbered">
            <p><span class="statement-heading"><span
            class="statement-label">Proof</span>.</span><span
            class="statement-spah"> </span>We assume <span
            class="math inline">\(B_r\)</span> is a blade, since all of
            the above statements are linear in <span
            class="math inline">\(B_r\)</span>. That is, it suffices to
            show that the above statement is true for <span
            class="math inline">\(B_r=e_1 e_2 \ldots e_r\)</span>, for
            any choice of orthogonal vectors <span
            class="math inline">\(\qty{e_1,\ldots,e_r}\)</span>. We
            repeatedly apply <span class="math inline">\(ab = 2a \cdot b
            - ba\)</span>. Observe that <span class="math inline">\(aB_r
            = 2 (a \cdot e_1) e_2 \ldots e_r - e_1 a e_2 \ldots
            e_r\)</span>. We can repeatedly do this, shifting <span
            class="math inline">\(a\)</span> further back in the chain,
            to get: <span class="math display">\[
            aB_r = 2 \sum_{k=1}^r (-1)^{k+1} (a \cdot e_k) (e_1 \ldots
            \check{e}_k \ldots e_r) + (-1)^r B_r a,
            \]</span> where <span
            class="math inline">\(\check{e}_k\)</span> denotes the fact
            that <span class="math inline">\(e_k\)</span> is omitted
            from the product (i.e. <span class="math inline">\(e_1
            \check{e}_2 e_3 = e_1 e_3\)</span>). A simple rearrangement
            gives: <span id="eq:dot-expansion"><span
            class="math display">\[
            \frac{1}{2} \qty(aB_r - (-1)^r B_r a) = \sum_{k=1}^r
            (-1)^{k+1} (a \cdot e_k) (e_1 \ldots \check{e}_k \ldots e_r)
            \tag{2}\]</span></span> Note that in the sum, <span
            class="math inline">\(e_1 \ldots \check{e}_k \ldots
            e_r\)</span> is grade <span
            class="math inline">\(r-1\)</span>. So, <span
            class="math inline">\(\frac{1}{2} \qty(aB_r - (-1)^r B_r
            a)\)</span> is a linear combination of blades, each of which
            are grade <span class="math inline">\(r-1\)</span> – so it
            is on the whole a homogenous multivector of grade <span
            class="math inline">\(r-1\)</span>.</p>
            </div>
            <p>A similar result holds for the wedge product <span
            class="math inline">\(\wedge\)</span>.</p>
            <div id="wedge" class="statement theorem plain">
            <p><span class="statement-heading"><span
            class="statement-label">Theorem 3</span>.</span><span
            class="statement-spah"> </span>For any blade <span
            class="math inline">\(B_r = b_1 \wedge \ldots \wedge
            b_r\)</span> and any vector <span
            class="math inline">\(a\)</span>: <span
            class="math display">\[
            a \wedge b_1 \wedge \ldots \wedge b_r = \frac{1}{2}\qty(a
            B_r + (-1)^r B_r a)
            \]</span></p>
            </div>
            <div class="statement proof proof unnumbered">
            <p><span class="statement-heading"><span
            class="statement-label">Proof</span>.</span><span
            class="statement-spah"> </span>Let <span
            class="math inline">\(e_1, \ldots e_r\)</span> be an
            orthogonalization of <span class="math inline">\(b_1, \ldots
            b_r\)</span> using the same technique as <a
            href="#blade-product" title="Lemma 1">Lemma 1</a>, such that
            <span class="math inline">\(b_1 \wedge \ldots \wedge b_r =
            e_1 \ldots e_r\)</span>. Then, let <span
            class="math inline">\(a_\perp = a - \sum_{k=1}^r \beta_k
            e_k\)</span>, where: <span class="math display">\[
            \beta_k = \begin{cases}
            \frac{a \cdot e_k}{(e_k \cdot e_k)^2} &amp; \qq{if $e_k
            \cdot e_k \neq 0$} \\
            0 &amp;\qq{otherwise}
            \end{cases}
            \]</span> Letting <span class="math inline">\(a_\parallel =
            \sum_{k=1}^r \beta_k e_k\)</span>, we can write <span
            class="math inline">\(a = a_\parallel + a_\perp\)</span>,
            and it is now simple to see that <span
            class="math inline">\(a_\perp \cdot e_k = 0\)</span> for all
            <span class="math inline">\(k=1,\ldots,r\)</span>.
            Furthermore, since each of the <span
            class="math inline">\(e_k\)</span> are merely linear
            combinations of <span class="math inline">\(b_k\)</span>,
            <span class="math inline">\(a_\perp \cdot b_k=0\)</span> for
            all <span class="math inline">\(k=1,\ldots,r\)</span> as
            well. Therefore, <span class="math inline">\(a \wedge b_1
            \wedge \ldots \wedge b_r=a_\perp \wedge b_1 \wedge \ldots
            \wedge b_r\)</span>.</p>
            <p>Now, observe that: <span
            class="math display">\[\begin{align*}
            a_\parallel B_r + (-1)^r B_r a_\parallel = \sum_{k=1}^r
            \beta_k \qty((-1)^{k+1}(e_k)^2 e_1 \ldots \check{e}_k \ldots
            e_r + (-1)^r (-1)^{r-k} e_1 \ldots \check{e}_k \ldots e_r
            (e_k)^2) \\
            &amp;= 0
            \end{align*}\]</span> Therefore, using the crucial fact that
            <span class="math inline">\(a_\perp b_k = -b_k
            a_\perp\)</span>: <span
            class="math display">\[\begin{align*}
            a B_r + (-1)^r B_r a &amp;= a_\perp B_r + (-1)^r B_r a_\perp
            \\
            &amp;= \frac{1}{r!}\sum_{\sigma \in \mathbb{S}_r}
            (-1)^{\sigma}\qty(a_\perp b_{\sigma(1)} \ldots b_{\sigma(r)}
            + (-1)^r b_{\sigma(1)} \ldots b_{\sigma(r)} a_\perp) \\
            &amp;= \frac{2}{r!} \sum_{\sigma \in \mathbb{S}_r}
            (-1)^\sigma a_\perp b_{\sigma(1)} \ldots b_{\sigma(r)} \\
            &amp;= \frac{2}{(r+1)!} \sum_{k=1}^{r+1} \sum_{\sigma \in
            \mathbb{S}_r} (-1)^\sigma a_\perp b_{\sigma(1)} \ldots
            b_{\sigma(r)}
            \end{align*}\]</span> Now, observe that we are free to
            scramble <span class="math inline">\(a_\perp\)</span>
            somewhere into the <span class="math inline">\(k\)</span>th
            position of <span class="math inline">\(b_{\sigma(1)} \ldots
            b_{\sigma(r)}\)</span> – that is, <span
            class="math inline">\(a_\perp b_{\sigma(1)} \ldots
            b_{\sigma(r)}=(-1)^k b_{\sigma(1)} \ldots b_{\sigma(k-1)}
            a_\perp b_{\sigma(k)} \ldots b_{\sigma(r)}\)</span>. But
            then, viewing <span class="math inline">\(a_\perp\)</span>
            as the first element of the set <span
            class="math inline">\(S = [a_\perp, b_1, \ldots,
            b_r]\)</span>, so that we identify <span
            class="math inline">\(b_0 = a_\perp\)</span> and <span
            class="math inline">\(S \cong [0, \ldots, r]\)</span>, we
            see that the permutation that sends <span
            class="math inline">\(S\)</span> to <span
            class="math inline">\([b_{\sigma(1)},\ldots,b_{\sigma(k-1)},a_\perp,b_{\sigma(k)},\ldots,b_{\sigma(r)}]
            \cong
            [\sigma(1),\ldots,\sigma(k-1),0,\sigma(k),\ldots,\sigma(r)]\)</span>
            has parity <span class="math inline">\((-1)^k
            (-1)^\sigma\)</span>! Therefore, the double sum over <span
            class="math inline">\(\sum_{k=1}^{r+1} \sum_{\sigma \in
            \mathbb{S}_r}\)</span> can be rewritten as a sum over
            permutations mapping <span
            class="math inline">\([0,\ldots,r]\)</span> onto itself –
            that is, <span
            class="math inline">\(\mathbb{S}_{r+1}\)</span>. We are then
            left with: <span class="math display">\[\begin{align*}
            a B_r + (-1)^r B_r a &amp;= \frac{2}{(r+1)!} \sum_{\sigma
            \in \mathbb{S}_{r+1}} b_{\sigma(0)} \ldots b_{\sigma(r)} \\
            &amp;= 2 a \wedge b_1 \wedge \ldots \wedge b_r,
            \end{align*}\]</span> as required.</p>
            </div>
            <div class="statement corollary plain">
            <p><span class="statement-heading"><span
            class="statement-label">Corollary 1</span>.</span><span
            class="statement-spah"> </span>For any multivector <span
            class="math inline">\(B_r \in \mathcal{G}_n^r\)</span>, let
            <span class="math inline">\(a \wedge B_r\)</span> be defined
            by: <span id="eq:wedge"><span class="math display">\[
            a \wedge B_r \coloneqq \frac{1}{2} (aB_r + (-1)^r B_r a),
            \tag{3}\]</span></span> so that <span
            class="math inline">\(a \wedge B_r\)</span> is always a
            homogenous multivector of grade <span
            class="math inline">\(r+1\)</span>. That is, <span
            class="math inline">\(\wedge\)</span> raises grade by <span
            class="math inline">\(1\)</span>.</p>
            </div>
            <div class="statement proof proof unnumbered">
            <p><span class="statement-heading"><span
            class="statement-label">Proof</span>.</span><span
            class="statement-spah"> </span>This follows simply by
            decomposing <span class="math inline">\(B_r\)</span> into a
            linear combination of blades and applying <a href="#wedge"
            title="Theorem 3">Theorem 3</a> to each blade (observing
            that <span class="math inline">\(a \wedge B_r\)</span> is
            bilinear)</p>
            </div>
            <p>It follows trivially from the definitions in <a
            href="#eq:dot">Equation 1</a> and <a
            href="#eq:wedge">Equation 3</a> that <span
            class="math inline">\(a B_r=a \cdot B_r + a \wedge
            B_r\)</span>, which says that the product <span
            class="math inline">\(aB_r\)</span> is a sum of homogenous
            multivectors with grade <span class="math inline">\(r \pm
            1\)</span>. This can be generalized for a general
            multivector <span class="math inline">\(A_s\)</span>. Since
            <span class="math inline">\(A_s\)</span> can be written as a
            linear combination of blades, and each blade can be written
            as a product of anticommuting (orthogonal) vectors, <span
            class="math inline">\(A_s B_r\)</span> is composed of
            multivectors with grades <span
            class="math inline">\(\abs{r-s}, \abs{r-s}+2, \ldots,
            r+s\)</span>. To further generalize the dot and wedge
            product notation, when we write <span
            class="math inline">\(A_s \cdot B_r\)</span>, we mean the
            lowest grade component of <span class="math inline">\(A_s
            B_r\)</span>, and the highest for <span
            class="math inline">\(A_s \wedge B_r\)</span>: <span
            class="math display">\[\begin{align}
            A_s \cdot B_r &amp;= \expval{A_s B_r}_{\abs{s-r}} \\
            A_s \wedge B_r &amp;= \expval{A_s B_r}_{s+r}
            \end{align}\]</span> We also define a scalar product <span
            class="math inline">\(\expval{A_1 A_2 \ldots A_k}\)</span>
            to be the grade 0 (i.e., scalar) component of the product
            <span class="math inline">\(A_1 A_2 \ldots A_k\)</span>.</p>
            <div id="wedge-associative" class="statement lemma plain">
            <p><span class="statement-heading"><span
            class="statement-label">Lemma 4</span>.</span><span
            class="statement-spah"> </span>The wedge product is
            associative: <span class="math display">\[
            (A_r \wedge B_s) \wedge C_t = A_r \wedge (B_s \wedge C_t),
            \]</span> so we can write <span class="math inline">\(A_r
            \wedge B_s \wedge C_t\)</span> unambiguously.</p>
            </div>
            <div class="statement proof proof unnumbered">
            <p><span class="statement-heading"><span
            class="statement-label">Proof</span>.</span><span
            class="statement-spah"> </span>Recall that we can expand
            <span class="math display">\[A_r B_s = \expval{A_r
            B_s}_{r+s} + \expval{A_r B_s}_{r+s-2} + \ldots + \expval{A_r
            B_s}_{\abs{r-s}}.\]</span> Since <span
            class="math inline">\((A_r \wedge B_s) \wedge C_t =
            \expval{A_r B_s}_{r+s} \wedge C_t\)</span>: <span
            class="math display">\[\begin{align*}
            \expval{A_r B_s}_{r+s} C_t &amp;= \qty(\expval{A_r
            B_s}_{r+s} C_t)_{r+s+t} + \qty(\expval{A_r B_s}_{r+s}
            C_t)_{r+s+t-2} + \ldots \\
            &amp;= \expval{(A_r B_s) C_t}_{r+s+t} + \ldots,
            \end{align*}\]</span> where the <span
            class="math inline">\(\ldots\)</span> means all remaining
            terms have lower grade. Therefore, <span
            class="math inline">\((A_r \wedge B_s) \wedge
            C_t=\expval{(A_r B_s) C_t}_{r+s+t}\)</span>. Finally, by
            associativity of the geometric product (recall, this is one
            of the axioms), <span class="math inline">\(\expval{(A_r
            B_s) C_t}_{r+s+t}=\expval{A_r (B_s C_t)}_{r+s+t}=A_r \wedge
            (B_s \wedge C_t)\)</span>.</p>
            </div>
            <div id="trip-prod" class="statement lemma plain">
            <p><span class="statement-heading"><span
            class="statement-label">Lemma 5</span>.</span><span
            class="statement-spah"> </span>For any vectors <span
            class="math inline">\(a\)</span>, <span
            class="math inline">\(b_1,\ldots,b_r\)</span>: <span
            class="math display">\[
            a \cdot (b_1 \wedge \ldots \wedge b_r) = \sum_{k=1}^r
            (-1)^{k+1} (a \cdot b_k) b_1 \ldots \check{b}_k \ldots b_r.
            \]</span></p>
            </div>
            <div class="statement proof proof unnumbered">
            <p><span class="statement-heading"><span
            class="statement-label">Proof</span>.</span><span
            class="statement-spah"> </span>Since <span
            class="math display">\[
            a \cdot (b_1 \ldots b_r) = a \cdot \expval{b_1 \ldots b_r}_r
            + a \cdot \expval{b_1 \ldots b_r}_{r-2} + \ldots,
            \]</span> by taking the <span
            class="math inline">\(r-1\)</span>th component of each side,
            we find: <span class="math display">\[
            \expval{a \cdot (b_1 \ldots b_r)}_{r-1} = a \cdot
            \expval{b_1 \ldots b_r}_r,
            \]</span> and since <span class="math inline">\(\expval{b_1
            \ldots b_r}_r=b_1 \wedge \ldots \wedge b_r\)</span>: <span
            class="math display">\[
            a \cdot (b_1 \wedge \ldots \wedge b_r) =
            \frac{1}{2}\expval{a b_1 \ldots b_r - (-1)^r b_1 \ldots b_r
            a}_{r-1}
            \]</span> Repeatedly applying the identity <span
            class="math inline">\(ab = 2 (a \cdot b) - ba\)</span>:
            <span class="math display">\[\begin{align*}
            a \cdot (b_1 \wedge \ldots \wedge b_r) &amp;=
            \expval{\sum_{k=1}^r (-1)^{k+1} (a \cdot b_k) b_1 \ldots
            \check{b}_k \ldots b_r}_{r-1} \\
            &amp;= \sum_{k=1}^r (-1)^{k+1} (a \cdot b_k) b_1 \ldots
            \check{b}_k \ldots b_r
            \end{align*}\]</span></p>
            </div>
            <div class="statement remark plain">
            <p><span class="statement-heading"><span
            class="statement-label">Remark 1</span>.</span><span
            class="statement-spah"> </span>The above result (<a
            href="#trip-prod" title="Lemma 5">Lemma 5</a>) is a
            generalization of the vector triple product identity in
            <span class="math inline">\(\mathbb{R}^3\)</span> which says
            <span class="math inline">\(a \cross (b \cross c)=(a \cdot
            c)b - (a \cdot b)c\)</span>. The analogy is <em>not</em>
            with <span class="math inline">\(a \cdot (b \cross
            c)\)</span>, because in three dimensions, cross product
            between a vector and a pseudovector <span
            class="math inline">\(b \cross c\)</span> is actually a dot
            product the language of geometric algebra. This fact
            originates in the fact that the pseudovector <span
            class="math inline">\(b \cross c\)</span> is really a
            bivector, and ordinary vector algebra lacks the language the
            make this fact clear. We discuss this further below.</p>
            </div>
            <h2 data-number="1.2" id="a-few-additional-operations"><span
            class="header-section-number">1.2</span> A Few Additional
            Operations</h2>
            <p>We define the reverse of a product of vectors to be <span
            class="math display">\[
            (a_1 \ldots a_r)^\dagger = a_r \ldots a_1.
            \]</span> Therefore, for an arbitrary blade <span
            class="math inline">\(A_r\)</span>, by writing <span
            class="math inline">\(A_r=e_1 \ldots e_r\)</span>, we see
            that <span class="math inline">\(A_r^\dagger=(-1)^{r(r-1)/2}
            A_r\)</span>, since <span
            class="math inline">\(r(r-1)/2\)</span> swaps occur in order
            for a complete reversion <span class="math inline">\(e_1
            \ldots e_r \rightarrow e_r \ldots e_1\)</span>. If we
            enforce <span class="math inline">\((A + B)^\dagger =
            A^\dagger + B^\dagger\)</span>, we can easily see that for
            <em>any</em> homogenous multivector <span
            class="math inline">\(A\)</span> of grade <span
            class="math inline">\(r\)</span> (not just blades), <span
            class="math inline">\(A^\dagger = (-1)^{r(r-1)/2}
            A\)</span>.</p>
            <div class="statement lemma plain">
            <p><span class="statement-heading"><span
            class="statement-label">Lemma 6</span>.</span><span
            class="statement-spah"> </span>The scalar product is
            invariant under cyclic permutation: <span
            class="math display">\[
            \expval{A_1 A_2 \ldots A_k} = \expval{A_2 A_3 \ldots A_k
            A_1} = \ldots = \expval{A_k A_1 \ldots A_{k-1}}
            \]</span> We sometimes write <span class="math inline">\(A *
            B = \expval{AB}\)</span>.</p>
            </div>
            <div class="statement proof proof unnumbered">
            <p><span class="statement-heading"><span
            class="statement-label">Proof</span>.</span><span
            class="statement-spah"> </span>For any <span
            class="math inline">\(A,B\)</span>, we have <span
            class="math inline">\(\expval{AB}=\expval{AB}^\dagger\)</span>
            since the grade of <span
            class="math inline">\(\expval{AB}\)</span> is <span
            class="math inline">\(r=0\)</span>, so <span
            class="math inline">\((-1)^{r(r-1)/2}=1\)</span>. By the
            linearity of the reversion operator, <span
            class="math inline">\(\expval{AB}^\dagger=\expval{(AB)^\dagger}\)</span>.
            This can be seen by writing <span
            class="math inline">\(AB\)</span> as a sum of homogenous
            multivectors, and applying <span
            class="math inline">\(^\dagger\)</span> to both sides. Since
            <span class="math inline">\((AB)^\dagger=B^\dagger
            A^\dagger\)</span>, we have <span
            class="math inline">\(\expval{AB}=\expval{B^\dagger
            A^\dagger}\)</span>. Finally, we observe that if the grade
            of <span class="math inline">\(A\)</span> and <span
            class="math inline">\(B\)</span> is <span
            class="math inline">\(r\)</span> and <span
            class="math inline">\(s\)</span> respectively, the lowest
            grade component of <span class="math inline">\(AB\)</span>
            is <span class="math inline">\(\abs{r-s}\)</span>.
            Therefore, if <span class="math inline">\(r \neq s\)</span>,
            <span
            class="math inline">\(\expval{AB}=0=\expval{BA}\)</span>. If
            <span class="math inline">\(r=s\)</span>, then <span
            class="math inline">\(\expval{AB}=\expval{B^\dagger
            A^\dagger}=(-1)^{r(r-1)}\expval{BA}=\expval{BA}\)</span>.
            So, it holds in general that <span
            class="math inline">\(\expval{AB}=\expval{BA}\)</span>.</p>
            <p>Now, for any <span class="math inline">\(m\)</span>, we
            can choose <span class="math inline">\(A=A_1 \ldots
            A_m\)</span> and <span class="math inline">\(B=A_{m+1}
            A_{m+2} \ldots A_k\)</span> to find <span
            class="math inline">\(\expval{A_1 \ldots
            A_k}=\expval{A_{m+1}A_{m+2} \ldots A_k A_1 \ldots
            A_m}\)</span> for any <span class="math inline">\(m = 1,
            \ldots, k\)</span>. This gives the desired result.</p>
            </div>
            <p>Another product is the commutator <span
            class="math display">\[A \times B = \frac{1}{2}
            (AB-BA).\]</span> Note that for a bivector <span
            class="math inline">\(B\)</span> and a vector <span
            class="math inline">\(a\)</span>, the commutator is equal to
            the dot product: <span class="math display">\[
            B \times a = \frac{1}{2} (Ba - aB) =-a \cdot B
            =-\expval{aB}_1 =-\expval{B^\dagger a^\dagger}_1 =
            \expval{Ba}_1 = B \cdot a
            \]</span></p>
            <p>It turns out that this is a specific case of a more
            general property, which we state below.</p>
            <div id="comm-grade" class="statement lemma plain">
            <p><span class="statement-heading"><span
            class="statement-label">Lemma 7</span>.</span><span
            class="statement-spah"> </span>For any homogenous
            multivector <span class="math inline">\(A_r\)</span>, <span
            class="math inline">\(B \times A_r\)</span> is also a
            homogenous multivector of grade <span
            class="math inline">\(r\)</span>. Specifically, <span
            class="math inline">\(B \times
            A_r=\expval{BA_r}_r\)</span>.</p>
            </div>
            <div class="statement proof proof unnumbered">
            <p><span class="statement-heading"><span
            class="statement-label">Proof</span>.</span><span
            class="statement-spah"> </span>We decompose <span
            class="math inline">\(B A_r= \expval{BA_r}_{r-2} +
            \expval{BA_r}_r + \expval{BA_r}_{r+2}\)</span>. Now, we
            reverse both sides. <span
            class="math display">\[\begin{align*}
            -(-1)^{r(r-1)/2} A_r B &amp;= (-1)^{(r-2)(r-3)/2}
            \expval{BA_r}_{r-2} + (-1)^{r(r-1)/2}\expval{BA_r}_r +
            (-1)^{(r+1)(r+2)/2}\expval{BA_r}_{r+2} \\
            -A_r B &amp;= (-1)^{-2r+3} \expval{BA_r}_{r-2} +
            \expval{BA_r}_r + (-1)^{4r+1} \expval{BA_r}_{r+2} \\
            A_r B &amp;= \expval{BA_r}_{r-2} - \expval{BA_r}_r +
            \expval{BA_r}_{r+2}
            \end{align*}\]</span> Therefore, it is simple to see that
            <span class="math inline">\(\frac{1}{2} (B A_r - A_r B) =
            \expval{BA_r}_r\)</span>.</p>
            </div>
            <p><a href="#comm-grade" title="Lemma 7">Lemma 7</a> gives a
            useful identity: <span class="math display">\[
            B A_r = B \cdot A_r + B \times A_r + B \wedge A_r
            \]</span></p>
            <p>Finally, in spaces of a finite dimension <span
            class="math inline">\(n\)</span>, all multivectors with
            grade <span class="math inline">\(n\)</span> are unique up
            to a scalar multiplier. We define the pseudoscalar <span
            class="math inline">\(I\)</span> to be the unique
            multivector of grade <span class="math inline">\(n\)</span>
            such that <span
            class="math inline">\(\abs{I^2}=1\)</span>.<span
            class="sidenote-wrapper"><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span
            class="sidenote">The sign of <span
            class="math inline">\(I^2\)</span> depends on the dimension
            <span class="math inline">\(n\)</span> and the signature of
            the space.<br />
            <br />
            </span></span> The pseudoscalar defines an orientation for
            the space – that is, any blade will have either the same or
            different sign as <span class="math inline">\(I\)</span>,
            which corresponds to a positive or negative orientation,
            respectively. The pseudoscalar is useful because it defines
            a duality transformation mapping blades to their orthogonal
            component <span class="math inline">\(A_r \rightarrow I
            A_r\)</span>. Note that <span class="math inline">\(I
            A_r\)</span> has grade <span
            class="math inline">\(n-r\)</span> because the <span
            class="math inline">\(r\)</span> basis vectors from <span
            class="math inline">\(A_r\)</span> combine with <span
            class="math inline">\(r\)</span> components of <span
            class="math inline">\(I\)</span>, leaving <span
            class="math inline">\(n-r\)</span> basis vectors. Because of
            this duality transformation, as one might expect, it can
            also be used to transform between the dot and wedge
            product.</p>
            <div id="duality" class="statement lemma plain">
            <p><span class="statement-heading"><span
            class="statement-label">Lemma 8</span>.</span><span
            class="statement-spah"> </span>For any multivectors <span
            class="math inline">\(A_r, B_s\)</span> such that <span
            class="math inline">\(r+s \leq n\)</span>: <span
            class="math display">\[
            A_r \cdot (B_s I) = A_r \wedge B_s I.
            \]</span></p>
            </div>
            <div class="statement proof proof unnumbered">
            <p><span class="statement-heading"><span
            class="statement-label">Proof</span>.</span><span
            class="statement-spah"> </span><span
            class="math display">\[\begin{align*}
            A_r \cdot (B_s I) &amp;= \expval{A_r B_s I}_{\abs{r-(n-s)}}
            \\
            &amp;= \expval{A_r B_s I}_{n-(r+s)} \\
            &amp;= \expval{A_r B_s}_{r+s} I \\
            &amp;= A_r \wedge B_s I
            \end{align*}\]</span></p>
            </div>
            <h2 data-number="1.3" id="rotations-and-reflections"><span
            class="header-section-number">1.3</span> Rotations and
            Reflections</h2>
            <p>Vectors can be reflected with respect to some direction
            (i.e. unit vector) <span class="math inline">\(n\)</span>.
            This reflection ought to take <span class="math inline">\(n
            \rightarrow -n\)</span>, and leave all vectors perpendicular
            to <span class="math inline">\(n\)</span> unchanged.
            Finally, it ought to be linear. The map <span
            class="math display">\[f(a) = -nan\]</span> does exactly
            this. We see that <span
            class="math inline">\(f(n)=-nnn=-n\)</span>, and for vectors
            <span class="math inline">\(n_\perp\)</span> orthogonal to
            <span class="math inline">\(n\)</span>, <span
            class="math inline">\(f(n_\perp)=-n n_\perp n = nn
            n_\perp=n_\perp\)</span>. This also ensures that for any
            vector <span class="math inline">\(a\)</span>, <span
            class="math inline">\(f(a)\)</span> is still a vector.</p>
            <p>Vectors can also be rotated with respect to some plane.
            This plane is described by two unit basis vectors <span
            class="math inline">\(n\)</span> and <span
            class="math inline">\(m\)</span>, or by the bivector <span
            class="math inline">\(n \wedge m\)</span>. Two successive
            reflections about vectors <span
            class="math inline">\(m\)</span> then <span
            class="math inline">\(n\)</span> results in a rotation. That
            is, <span class="math display">\[f(a)=nmamn\]</span> is a
            rotation. It is trivial to see that this leaves vectors
            perpendicular to both <span class="math inline">\(n\)</span>
            and <span class="math inline">\(m\)</span> unchanged. We
            also verify that <span class="math inline">\(f(a) \cdot f(b)
            = a \cdot b\)</span>, as required by definition of a
            rotation. <span class="math display">\[\begin{align*}
            f(a) \cdot f(b) &amp;= \frac{1}{2} \qty(nmamn nmbmn + nmbmn
            nmamn) \\
            &amp;= \frac{1}{2} \qty(nm ab mn + nm ba mn) \\
            &amp;= nm (a \cdot b) mn \\
            &amp;= a \cdot b
            \end{align*}\]</span> We call the product <span
            class="math inline">\(R=nm\)</span> a rotor, and therefore
            rotors generate rotations via <span class="math inline">\(R
            a R^\dagger\)</span>. Since rotations form a group,
            composing rotations <span class="math inline">\(a
            \rightarrow R_1 R_2 a R_2 R_1\)</span> results in another
            rotation. Therefore, we say that <span
            class="math inline">\(R_1 R_2\)</span> is also a rotor. That
            is, rotors also form a group. In general, rotors are simply
            geometric products of an even number of vectors with the
            normalization condition <span class="math inline">\(R
            R^\dagger = 1\)</span>.</p>
            <p>This description of rotation leads to interesting
            behavior on multivectors. For instance, take some geometric
            product <span class="math inline">\(a_1 \ldots a_k\)</span>.
            If we rotate each of the component vectors, we get <span
            class="math inline">\((Ra_1R^\dagger)(R a_2 R^\dagger)
            \ldots (R a_k R^\dagger) = R a_1 \ldots a_k
            R^\dagger\)</span>. We get similar behavior for a blade
            (since the blade is just a linear combination of geometric
            products). Therefore, the rotation transformation for
            arbitrary multivectors <span
            class="math inline">\(A\)</span> is still <span
            class="math inline">\(R A R\)</span>.</p>
            <h2 data-number="1.4" id="frames"><span
            class="header-section-number">1.4</span> Frames</h2>
            <p>A set of linearly independent vectors <span
            class="math inline">\(\qty{\vb{e_1}, \ldots,
            \vb{e_n}}\)</span> is called a frame in geometric algebra.
            We do not necessarily assume these are orthonormal vectors,
            so they do not (in general) commute. We define the volume
            element <span class="math display">\[E_n=\vb{e}_1 \wedge
            \ldots \wedge \vb{e}_n, \]</span> and the reciprocal frame
            <span
            class="math inline">\(\qty{\vb{e}^1,\ldots,\vb{e}^n}\)</span>
            by <span id="eq:recip"><span class="math display">\[
            \vb{e}_j \cdot \vb{e}^k = \delta_j^k.
            \tag{4}\]</span></span> We can write an explicit form for
            the reciprocal vectors: <span class="math display">\[
            \vb{e}^k = (-1)^{k+1} (\vb{e}_1 \wedge \ldots \wedge
            \check{\vb{e}}_k \wedge \ldots \vb{e}_n) E_n^{-1}.
            \]</span> Recall the <span
            class="math inline">\(\check{\vb{e}}_k\)</span> means that
            this term is omitted. Intuitively, what this says is that
            <span class="math inline">\(\vb{e}^k\)</span> is orthogonal
            to each basis vector except <span
            class="math inline">\(\vb{e}_k\)</span>, and we add the
            inverse volume element <span
            class="math inline">\(E_n^{-1}\)</span> at the end to
            normalize. More rigorously, we can verify that this
            definition satisfies <a href="#eq:recip">Equation 4</a> by
            making use of the duality transformation in <a
            href="#duality" title="Lemma 8">Lemma 8</a>. <span
            class="math display">\[\begin{align*}
            \vb{e}_j \cdot \vb{e}^k &amp;= \vb{e}_j \cdot
            \qty((-1)^{k+1} (\vb{e}_1 \wedge \ldots \wedge
            \check{\vb{e}}_k \wedge \ldots \vb{e}_n) E_n^{-1}) \\
            &amp;= (-1)^{k+1} \vb{e_j} \wedge \vb{e}_1 \wedge \ldots
            \wedge \check{\vb{e}}_k \wedge \ldots \vb{e}_n E_n^{-1}
            \end{align*}\]</span> Note that if <span
            class="math inline">\(j \neq k\)</span>, the wedge product
            vanishes. Otherwise, <span class="math inline">\((-1)^{k+1}
            \vb{e_j} \wedge \vb{e}_1 \wedge \ldots \wedge
            \check{\vb{e}}_k \wedge \ldots \vb{e}_n = E_n\)</span>.
            Therefore, we recover our desired result <span
            class="math inline">\(\vb{e}_j \cdot \vb{e}^k =
            \delta_j^k\)</span>.</p>
            <p>Once we have chosen a frame for our space, we can write
            vectors as linear combinations of the frame basis vectors:
            <span class="math display">\[a = a^i \vb{e}_i = a_i
            \vb{e}^i,\]</span> where we are using Einstein summation
            notation. By the reciprocal property in <a
            href="#eq:recip">Equation 4</a>, we can see that <span
            class="math inline">\(a^i = a \cdot \vb{e}^i\)</span> and
            <span class="math inline">\(a_i = a \cdot \vb{e}_i\)</span>.
            Therefore, <span class="math inline">\(a \cdot \vb{e}_i
            \vb{e}^i = a \cdot \vb{e}^i \vb{e}_i = a\)</span> (recall
            that the order of operations is <span
            class="math inline">\(\cdot\)</span> and <span
            class="math inline">\(\wedge\)</span> before the geometric
            product). This is an instance of a more general fact.</p>
            <div class="statement lemma plain">
            <p><span class="statement-heading"><span
            class="statement-label">Lemma 9</span>.</span><span
            class="statement-spah"> </span>For any frame <span
            class="math inline">\(\qty{\vb{e}_i}\)</span> and any
            homogenous multivector <span
            class="math inline">\(A_r\)</span>, <span
            class="math display">\[\vb{e}_i \vb{e}^i \cdot A_r = r
            A_r.\]</span></p>
            </div>
            <div class="statement proof proof unnumbered">
            <p><span class="statement-heading"><span
            class="statement-label">Proof</span>.</span><span
            class="statement-spah"> </span>We apply <a
            href="#eq:dot-expansion">Equation 2</a>. We also assume
            <span class="math inline">\(A_r\)</span> is a blade that can
            be written as a product of orthogonal vectors <span
            class="math inline">\(\vb{a}_1 \vb{a}_2 \ldots
            \vb{a}_r\)</span> (the general case follows trivially
            because the desired identity is linear in <span
            class="math inline">\(A_r\)</span>). <span
            class="math display">\[\begin{align*}
            \vb{e}_i \vb{e}^i \cdot A_r &amp;= \vb{e}_i \sum_{k=1}^r
            (-1)^{k+1} (\vb{e}^i \cdot \vb{a}_k) (\vb{a}_1 \ldots
            \check{\vb{a}}_k \ldots \vb{a}_r) \\
            &amp;= \sum_{k=1}^r (-1)^{k+1} \vb{a}_k \vb{a}_1 \ldots
            \check{\vb{a}}_k \ldots \vb{a}_r \\
            &amp;= \sum_{k=1}^r \vb{a}_1 \ldots \vb{a}_r \\
            &amp;= r A_r
            \end{align*}\]</span></p>
            </div>
            <p>Since <span class="math inline">\(\qty{\vb{e}_1, \ldots,
            \vb{e}_n}\)</span> spans <span
            class="math inline">\(\mathbb{R}^n\)</span>, any blade of
            degree <span class="math inline">\(r\)</span> can be written
            as a linear combination of wedge products <span
            class="math inline">\(\vb{e}_i \wedge \ldots \wedge \vb{e}_j
            \wedge \vb{e}_k\)</span>. This holds similarly for the
            reciprocal frame <span class="math inline">\(\qty{\vb{e}^1,
            \ldots, \vb{e}^n}\)</span>. Therefore, any homogenous
            multivector <span class="math inline">\(A\)</span> of grade
            <span class="math inline">\(r\)</span> can be expanded:
            <span class="math display">\[
            A = \sum_{i_1 &lt; i_2 &lt; \ldots &lt; i_r} A_{i_1 \ldots
            i_r} \vb{e}^{i_1} \wedge \ldots \vb{e}^{i_2} \wedge
            \vb{e}^{i_r}
            \]</span> An explicit formula for the components <span
            class="math inline">\(A_{i_1 \ldots i_r}\)</span> can be
            given: <span class="math display">\[
            A_{i_1 \ldots i_r} = \expval{(e_{i_r} \wedge \ldots \wedge
            e_{i_1}) A}
            \]</span></p>
            <h1 class="unnumbered" id="bibliography">References</h1>
            <div id="refs"
            class="references csl-bib-body hanging-indent"
            role="doc-bibliography">
            <div id="ref-doran2007" class="csl-entry"
            role="doc-biblioentry">
            Doran, Chris, and A. N. Lasenby. 2007. <em>Geometric Algebra
            for Physicists</em>. 1st ed. Cambridge ; New York: Cambridge
            University Press.
            </div>
            </div>
      
      <div>
                <hr style="margin: 1em 0.2em 1em 60%" />

        <p class="date">Last modified: June 28, 2022</p>
              </div>
    </div>

  </div>
  <div style="position: fixed; bottom: 1em; right: 1em">
    <i class="fa fa-moon-o" aria-hidden="true" id="flip-theme"></i>
  </div>
</body>

</html>